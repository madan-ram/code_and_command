Common pitfall(taking mean on training, validation and testing dataset):
An important point to make about the preprocessing is that any 
preprocessing statistics (e.g. the data mean) must only be computed
 on the training data, and then applied to the validation / test data.
 E.g. computing the mean and subtracting it from every image across the
 entire dataset and then splitting the data into train/val/test splits 
 would be a mistake. Instead, the mean must be computed only over the 
 training data and then subtracted equally from all splits (train/val/test).

Pitfall (initialization parameter): 
all zero initialization. Lets start with what we should not do. 
Note that we do not know what the final value of every weight 
should be in the trained network, but with proper data normalization 
it is reasonable to assume that approximately half of the weights will
be positive and half of them will be negative. A reasonable-sounding idea 
then might be to set all the initial weights to zero, which we expect to 
be the "best guess" in expectation. This turns out to be a mistake, because
if every neuron in the network computes the same output, then they will also 
all compute the same gradients during backpropagation and undergo the exact
same parameter updates. In other words, there is no source of asymmetry between 
neurons if their weights are initialized to be the same.

batch and data shuffle[1]:
As for any stochastic gradient descent method (including
the mini-batch case), it is important for ef-
ficiency of the estimator that each example or minibatch
be sampled approximately independently. Because
random access to memory (or even worse, to
disk) is expensive, a good approximation, called incremental
gradient (Bertsekas, 2010), is to visit the
examples (or mini-batches) in a fixed order corresponding
to their order in memory or disk (repeating
the examples in the same order on a second epoch, if
we are not in the pure online case where each example
is visited only once). In this context, it is safer if
the examples or mini-batches are first put in a random
order (to make sure this is the case, it could
be useful to first shuffle the examples). Faster convergence
has been observed if the order in which the
mini-batches are visited is changed for each epoch,
which can be reasonably efficient if the training set
holds in computer memory.



[1] Practical Recommendations for Gradient-Based Training of Deep
Architectures
Yoshua Bengio


Additional Data Transformations for Training
	Imagenet classification system [6], three types of image transformations were used to
augment the training set. The first was to take a randomly located crop of 224x224 pixels
from a 256x256 pixel image capturing some translation invariance. The second was to flip
the image horizontally to capture the reflection invariance. The final data transformation was
to add randomly generated lighting which tries to capture invariance to the change in
lighting and minor color variation. We add additional transformations that extend the
translation invariance and color invariance.