Common pitfall(taking mean on training, validation and testing dataset):
An important point to make about the preprocessing is that any 
preprocessing statistics (e.g. the data mean) must only be computed
 on the training data, and then applied to the validation / test data.
 E.g. computing the mean and subtracting it from every image across the
 entire dataset and then splitting the data into train/val/test splits 
 would be a mistake. Instead, the mean must be computed only over the 
 training data and then subtracted equally from all splits (train/val/test).

Pitfall (initialization parameter): 
all zero initialization. Lets start with what we should not do. 
Note that we do not know what the final value of every weight 
should be in the trained network, but with proper data normalization 
it is reasonable to assume that approximately half of the weights will
be positive and half of them will be negative. A reasonable-sounding idea 
then might be to set all the initial weights to zero, which we expect to 
be the "best guess" in expectation. This turns out to be a mistake, because
if every neuron in the network computes the same output, then they will also 
all compute the same gradients during backpropagation and undergo the exact
same parameter updates. In other words, there is no source of asymmetry between 
neurons if their weights are initialized to be the same.

batch and data shuffle[1]:
As for any stochastic gradient descent method (including
the mini-batch case), it is important for ef-
ficiency of the estimator that each example or minibatch
be sampled approximately independently. Because
random access to memory (or even worse, to
disk) is expensive, a good approximation, called incremental
gradient (Bertsekas, 2010), is to visit the
examples (or mini-batches) in a fixed order corresponding
to their order in memory or disk (repeating
the examples in the same order on a second epoch, if
we are not in the pure online case where each example
is visited only once). In this context, it is safer if
the examples or mini-batches are first put in a random
order (to make sure this is the case, it could
be useful to first shuffle the examples). Faster convergence
has been observed if the order in which the
mini-batches are visited is changed for each epoch,
which can be reasonably efficient if the training set
holds in computer memory.



[1] Practical Recommendations for Gradient-Based Training of Deep
Architectures
Yoshua Bengio


Additional Data Transformations for Training {http://arxiv.org/pdf/1312.5402v1.pdf} 
	Imagenet classification system [6], three types of image transformations were used to
augment the training set. The first was to take a randomly located crop of 224x224 pixels
from a 256x256 pixel image capturing some translation invariance. The second was to flip
the image horizontally to capture the reflection invariance. The final data transformation was
to add randomly generated lighting which tries to capture invariance to the change in
lighting and minor color variation. We add additional transformations that extend the
translation invariance and color invariance.

translated image crops of 224x224 pixels were selected from a training image
of 256x256. The 256x256 image was generated by rescaling the largest image dimension to
256 and then cropping the other side to be 256. This results in a loss of information by not
considering roughly 30% of the pixels. While the cropped pixels are most likely less
informative than the middle pixels we found that making use of these additional pixels
improved the model.
To use the whole image, we first scale the smallest side to 256 leaving us with a 256xN or
Nx256 sized image. We then select a random crop of 224x224 as a training image. This
yields a large number of additional training examples and helps the net learn more extensive
translation invariance. Figure 1 shows a comparison of a square cropped image versus using
the full image. The square training image of the cat will never generate training examples
with a tail in it or with both ears compared to selecting crops from the full image

In addition to the random lighting noise that has been used in previous pipelines [6], we also
add additional color manipulations. We randomly manipulate the contrast, brightness and
color using the python image library (PIL). This helps generate training examples that cover
the span of image variations helping the neural network to learn invariance to changes in
these properties. We randomly choose an order for the three manipulations and then choose a
number between 0.5 and 1.5 for the amount of enhancement (a setting of 1 leaves the image
unchanged). After manipulating the contrast, brightness and color, we then add the random
lighting noise similar to [6].

Images contain useful predictive elements at different scales. To capture this we make
predictions at three different scales. We use the original 256 scale as well as 228 and 284.
Note that when scaling an image up, it is important to use a good interpolation method like
bicubic scaling and not to use anti aliasing filters designed for scaling images down. When
scaling down, anti aliasing seems to help a little bit but in practice we used bicubic scaling
for up scaling and down scaling.

In order to make use of all of the pixels in the image when making predictions, we generate
three different square image views. For an 256xN (Nx256) image, we generate a left (upper),
center, and right (lower) view of 256x256 pixels and then apply all crops, flips and scales to
each of these views. Figure 2 demonstrates how the three views are constructed.
Table 1 shows the effect of using the new training and testing transforms compared to the
previous baseline result. It also shows that the new model architecture with doubled fully
connected layers does not improve the top 5 error rate.

